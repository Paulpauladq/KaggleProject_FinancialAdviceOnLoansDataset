{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS6140 Assignments\n",
    "\n",
    "**Instructions**\n",
    "1. In each assignment cell, look for the block:\n",
    " ```\n",
    "  #BEGIN YOUR CODE\n",
    "  raise NotImplementedError.new()\n",
    "  #END YOUR CODE\n",
    " ```\n",
    "1. Replace this block with your solution.\n",
    "1. Test your solution by running the cells following your block (indicated by ##TEST##)\n",
    "1. Click the \"Validate\" button above to validate the work.\n",
    "\n",
    "**Notes**\n",
    "* You may add other cells and functions as needed\n",
    "* Keep all code in the same notebook\n",
    "* In order to receive credit, code must \"Validate\" on the JupyterHub server\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7c336275b386d6439ea67e77e96d9045",
     "grade": false,
     "grade_id": "cell-d21a86d292a3d7ef",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Final Project Report\n",
    "\n",
    "Write your final project report in the cells below. All written material must be completed prior to your presentation slot. Including figures, appendices, etc. a printed version of this document should be no more than 10 pages and no less than 6 pages. \n",
    "\n",
    "The following cell is used for overall feedback and deductions for length, content, and style."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "ce7ea5533b1effff2e13157180854ce3",
     "grade": true,
     "grade_id": "cell-343bed4feca7930f",
     "locked": false,
     "points": 10,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Introduction\n",
    "\n",
    "_(Why would people want to study this dataset and what is the primary task. Find out what the \"target\" variable means and why the customer is interested in running a competition on this dataset.) 2-3 paragraphs_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "1b7d89f8e38774cd0b2d59d09f04a82b",
     "grade": true,
     "grade_id": "cell-c9bb94ed861b91bf",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "  The dataset is basically about the loan appliation request the applicants have made. It strives to broaden financial inclusion for the unbanked population by providing a positive and safe borrowing experience. In order to make sure this underserved population has a positive loan experience, Home Credit makes use of a variety of alternative data--including telco and transactional information--to predict their clients' repayment abilities. We have to predict whether the bank should approve the loan according to all information of the applicants. Considering the main table, there're unique SK_ID_CURR which is the main key represents the id of applicants and the entire dataset is divided into two parts -- training set and test set. \n",
    "  In the training set, there's TARGET field represents the \"label\" of the dataset. If the TARGET field is 1, it means client with payment difficulties and the bank should not approve the loan application. Otherwise the bank should approve the application. On the other hand, in the testing set, there's no TARGET label so the dataset is used to set as ground truth to compare with prediction value and plot the AUC curve to determine whether the model is good or not. \n",
    "  After training the dataset with the chosen model and making good predictions on test set, we can have a good guess of whether the loan application is successfully or not in advance. People can use this model and feed the existing data to the model to predict the result before they apply and avoid unneccessary applications. Our Kaggler are challenging to help them unlock the full potential of their data, this will ensure that clients capable of repayment are not rejected and that loans are given with a principal, maturity, and repayment calendar that will empower their clients to be successful.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Propsed method\n",
    "_(Present an 1-paragraph description of your method and why you believe it is better that the other things you have tried)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "ad319c78971c6e4155af9a1e0aa2cf64",
     "grade": true,
     "grade_id": "cell-6085b28e8af4d158",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "  Generally speaking, the method I apply to the dataset can be separated to 3 parts -- data preprocessing and analysis, feature selection and training with selected model. For the first part, I apply some data preprocessing method to the dataset, to select some most important features. Firstly I downloaded the data from Kaggle and have a preview of all the tables and read table HomeCredit_columns_description so that I can have the intuition of the entire dataset and the relationship between different tables. After that, I did some further exploration to dig deeper to the dataset. For each table, I break down every single fields and group them into numerical fields or categorical fields. Furthermore, I also paid attention to whether the columns are sparse or dense as well as some property of some data columns. On to the next step, which is the most important step, I created some features based on the data analysis. What I really did is selected not only some important numerical features but also some categorical featuers. I created the new feature using operations like plus, minus, sum, average. Also, I paid crucial attention to missing values, the distribution of the data as well as the possibility to transform the data and did some corresponding operations like fill mean values, normalization as well as data transformation. Afterwards, I put the newly-constructed features into the dataset to be trained. Last but not least, it's model selection and training. I tried every model including different regression model and also the decisition tree model. And finally I selected Logistic Regression Model and used Stochastic Gradient Descent to train the model because it has better performance compared to other models. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Related Work\n",
    "_(Find (5-6) examples of people who have worked on similar dataset from the literature. Note: Literature == Published paper in a conference (not stack overflow). Briefly describe in 1-2 sentences the kinds of features, algorithms, or other methods they applied. Also explain why you believe your method is better. Provide a numbered reference id that will appear later in the references section. 3-5 paragraphs_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "ede8e26b7174b9d514f4cf789af54bde",
     "grade": true,
     "grade_id": "cell-299d7142a9907f1d",
     "locked": false,
     "points": 10,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "[7] A Data-Driven Approach to Predict the Success of Bank Telemarketing.\n",
    "\n",
    "They propose a data mining (DM) approach to predict the success of telemarketing calls for selling bank long-term deposits. A Portuguese retail bank was addressed, with data collected from 2008 to 2013, thus including the effects of the recent financial crisis. We analyzed a large set of 150 features related with bank client, product and social-economic attributes\n",
    "\n",
    "They used four DM models: logistic regression, decision trees, neural network and support vector machine.\n",
    "\n",
    "[8] Weight-Selected Attribute Bagging for Credit Scoring\n",
    "\n",
    "In this paper, they propose an improved attribute bagging method, weight-selected attribute bagging, to evaluate credit risk.\n",
    "\n",
    "[9] An Overview of Personal Credit Scoring: Techniques and Future Work\n",
    "\n",
    "In this paper, the techniques used for credit scoring are summarized and classified and the new methodâ€”ensemble learning model is introduced. \n",
    "\n",
    "[10] A Hybrid Online Sequential Extreme Learning Machine with Simplified Hidden Network\n",
    "\n",
    "In this paper, a novel learning algorithm termed Hybrid Online Sequential Extreme Learning Machine is proposed. The proposed HOS-ELM algorithm is a fusion of the Online Sequential Extreme Learning Machine and the Minimal Resource Allocation Network. \n",
    "\n",
    "[11] Managerial applications of neural networks: the case of bank failure predictions\n",
    "\n",
    "They used discrimant functions from a variable space on a binary classification problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Related implementations\n",
    "_(Find (2-3) examples of what people in Kaggle have done on this particular dataset [[2]](https://www.kaggle.com). Reference the URL of their kernel, post, etc. Describe in 1-2 sentences what they have done and why you think your method is better.) 2-3 paragraphs_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "5b53e3ee2a6b1cb541dd35a70f821025",
     "grade": true,
     "grade_id": "cell-a30ca527d1596437",
     "locked": false,
     "points": 10,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "1)The first link[[1]] refers to the 1st place solution on the result board. At first, the author made some analysis about the dataset and consider this problem as one of most complex problems for applying machine learning to. About the feature engineering, they discovered many amazing features that maybe ignored easily. For instance, the members of their team found several useful features like the  region id, the sum of all credit debt over the sum of all credit, credit annuity ratio and integer form of actual age. What they do is just some simple calcalution, but what is important is that we combine the single features that has meaningful connections with others. I used a lot of features they had discovered and achieve relatively good information gain. The number of features they used to combine and train is over 200, but I only used about 20 because of limit of time but still achieve good result. As for the model selection, they used ensembling algorithms like XGBoost, LightGBM and a Hill Climber linear model.\n",
    "\n",
    "2)The second link[[2]] refers to another solution of the problem. The author posts some simple lines of his work and corresponding code to work on the dataset. What he actually do is select relatively single feature and apply it to the training process. There're lots of similarity between the feature selection process between this method and previous one which also gave me lots of inspirations. So the key ideas the author used is to divide or subtract important features to get rates or values that are more meaningful (like annuity rate and income). Also, in bureau table, he create features for active credits and closed credits, in previous application table he creates specific features for approved and refused applications. Other than that, for all the categorical features, the author uses one-hot encoding. Last but not least he uses one function for each table to achieve modularity which I thought it's also very useful to apply. Last but most important, about the model selection,the author uses LightGBM with K-fold or Stratfied KFold. He implemented the whole solution is Python and got relatively good rankings.\n",
    "\n",
    "3)The third post[[3]] is mainly about the data preprocessing. It focuses on retrieving some samples of data and do some categorization of the data. First several steps about the data is to retrieve the data and take a glimpse of data.It extract the size of the data and retrieve several important features of each table and count the number of missing data of each column in each table. After the initial steps, it dig deeper into the data and find the most important single features from the main table since the main table has the most influence with the data. Furthermore, he uses some plot tools to plot the distribution of some crucial single features like AMT_GOOD_PRICE, AMT_INCOME_TOTAL and check whether the data is balanced or not. Then he plots some pie chart and colume chart to make some component categorical features more clear to understand such as the purpose of loan, income sources of applicant's who applied for loan, family status of applicant's who applied for loan, etc. On top of that, pearson correlation of features is also very important in this article. He also plotted the correlation plot, this can avoid the internal correlation when creating new features. Though this article doesn't mention the entire pipeline of dealing with the solution, it gave a very \n",
    "detailed presentation of the data prepocessing which is very useful if applying it  to feature selection method.\n",
    "\n",
    "4)Just like the previous article, this article[[4]] focuses on the specific aspects instead of the entire pipeline. It focuses on the specific table -- Burean data. Basically, the author explains the meaning of the dataset -- the loan data of each unique costumer with all financial institutions other than Home Credit. Also, for each unique SK_ID_CURR we have multiple SK_ID_BUREAU ID's, each being a unique loan transaction from other financial institutions availed by the same customer and reported to the bureau. The authors provides several features regarding Bureau table. He created 3 categorizations -- credit profile, how was credit consumed in the past and potential future delinquencies. For the first categorization, it includes number of past loans per customer, number of types of past loans per customer, etc. For the second, it includes average number of days between successive past applications for each customer, % of loans per customer where end data for credit is past. For the last one, it contains average number of days in which credit expires in future, ratio of total debt to total credit for each customer. This article is proposed to be very useful because it generates several features in the bureau table. What I really do is taking several features and giving it a try to extract component features joining the main table and bureau table. And it turns out it helps a lot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Data Analysis\n",
    "_(Data Analysis: Describe the data analysis you have completed, include 1-2 plots of the most useful features or learnings you have obtained from the dataset. Do not include the code, but do include formulas to anything you have calculated such as different feature combinations, feature selection, or analysis methods. You must use at least one clustering algorithm we have seen in class for an analysis of the data. Provide a link to the specific notebook cell in previous notebooks as a reference.)_ 5-6 paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "dadf3d2d4ff41cb2e933d964f940c8b4",
     "grade": true,
     "grade_id": "cell-f6715c211519644e",
     "locked": false,
     "points": 10,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "  For the data analysis part, the basic method I used contains three parts -- overview of the dataset, evaluation and comparsion of each single features and mix and composition of the features.\n",
    "  \n",
    "  For the first step, what I did is downloading the dataset first and traversing each one of the fields to understand what each of them stands for. This gave me the entire understanding of the dataset, somthing like what it can do and why we want to deal with such datasets. Getting to know the high-level meaning of the dataset is very cruical and can give you more intuition when go on to the next steps.\n",
    "  \n",
    "  For the second step, what I did is using Python to extract some important properties of the dataset such as the size of the each tables, the number of missing values, how the table is connected to other tables. The description on the Kaggle have alreay provided the dataset relationship chart. So I found that nearly all the tables are connected to the main table using the one-to-may mapping on the field SK_ID_CURR. So when we extract different columns from the other tables, what we really want to do is to use left join and group it by SK_ID_CURR. After that, I simply performed some SQL operations on the dataset and got to know the data better. After some operations, I'm able to plot some pie chart and column cart according to specific fields. So I split the fields into categorical fields and numeric fields. For the former, I plotted some pie charts to indicate the distribution and percentage of each categories. For instance, we can indicate whether the previous application is X-sell, walk-in or XNA. According to the pie chart below, the XNA is 63.7%, the X-sell is 27.3% and walk-in is 9%. \n",
    "  \n",
    "<img src = \"Pie1.JPG\">\n",
    "\n",
    "  For the latter, I plotted some distribution plot for the continuous value. For instance, below is the distribution of AMT_CREDIT field. As you can see, the data clusters at the range between 500k and 1000k.Using these plots, we can have better understand of each fields and do suitable operations correspondingly. For example, if the two data both have the normal distribution, it's more likely that we generate a more sigificant feature combining them together, otherwise the combining them may result in worse information gain than using single features. This really helped when I created the new features.\n",
    "  \n",
    "<img src = \"dist1.JPG\">\n",
    "\n",
    "  Since the features in the main table are of greatest value and importance, I used random forest to sort the importance of all the features. The importance plot is as follows. This plot means everything when you mix up the feature. Adding the single features with most importance into the newly-created features will have better information gain and thus can gain better accuracy.\n",
    "  For the last step, I combine some numerical fields and add add, subtraction, multiply, average operations into it. Also, I combine the relatively sigificant single categorical features in pairs and transform them into the continuous numeric features. I gained a performance boost using these transformed features. Also, for the missing values, I tried both filling-zeros and filling-means, also I normalized the features after I created them. These operations worked well.\n",
    "  \n",
    "<img src = \"Plot1.JPG\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proposed Methods\n",
    "_(Describe the ML algorithms you used for questions [3.1](part-3.ipynb#Question-3.1), [4.1](part-3.ipynb#Question-4.1), and all others. Focus on the formulas, any feature extractions, parameter tuning, etc. Explain how the algorithm works. E.g., if you used a decision, don't say \"I used a decision tree\", explain briefly how a decision tree works and why it was ideally suited for the dataset you chose.)_ 3-5 paragraphs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "cdb42ea3806602e6aa3782bb2af3f2aa",
     "grade": true,
     "grade_id": "cell-616d5470b36b8952",
     "locked": false,
     "points": 10,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "  Firstly, I'll provide the list of my newly-created features as follows:\n",
    "  1. sum of credit sum over sum of debt_sum remix ext_2\n",
    "  2. avg of credit sum over avg of ann remix ext_2\n",
    "  3. days credit avg / power of days remix remix ext_2\n",
    "  4. amt credit over amt goods price remix ext_3\n",
    "  5. linear sum of all ext_source with weights\n",
    "  6. nonlinear remix of all ext_source with weights\n",
    "  7. mash up with income total credit and ext_source\n",
    "  8. creditdownpayment: AMTGOODPRICE - AMTCREDIT remix ext_source\n",
    "  9. age int remix ext_source\n",
    "  10. organization_type remix with name contract type\n",
    "  11. name_education_type remix with organization_type\n",
    "  12. name_family_status remix with occupation_type\n",
    "  13. name_housing_type remix with occupation_type\n",
    "  14. flag_own_realty remix with organization_type\n",
    "  15. code_gender remix with organization_type\n",
    "  \n",
    "  I used logistic regression with regularization [[5]], there're few reasons that I used logistic regression. Firstly, the features I extract are definitely non-linear, so I think the logistic regression will work better comparing to Linear Regression. Secondly, since we need to calculate the prediction scores of each rows, the regression method should be a very efficient way if we normalize the dataset.Actually I also tried decision tree since there're so many categorical features and can be divided into decision subtree. But the result is not as good as Logistic Regression and the speed is so slow. Thus I chose Logistic Regression at last.\n",
    "  \n",
    "  Since the features we provide are very complex, so the dataset I extract will have a high variance so it's a neccessity to use methods to prevent overfitting otherwise the model will have good results on the training set but will fail to predict the unseen dataset well. So I apply the regularization method to the Logistic Regression.\n",
    "  \n",
    "  As for the training process, I used Stochastic Gradient Descend with mini-batch, because I think Gradient Descent will use all rows of data whereas the Stochastic Gradient Descend can only use a batch of the samples. Since the training dataset is huge, I believe Stochastic Gradient Descent will perform much faster and more efficient. I tuned the learning rate and the batch-size for several times and finally determine to use 0.01 and 50, respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "_(Provide some insights as to why you think that the proposed algorithms and features are good for this dataset. Explain whether you believe these are general properties that might be helpful for similar datasets--what makes them similar and why. What about this dataset made your solution successful. Could we use this for other datasets, if so, what types and why?)_ 3-5 paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "9ae0fa2d0474dd5cc8c4555d77506c12",
     "grade": true,
     "grade_id": "cell-a5fad0534cafdb6f",
     "locked": false,
     "points": 10,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "  For the dataset similar to this one, I believe it should have many auxilliary tables and link to the main table using ID. \n",
    "  \n",
    "  Also, the features many contains both numerical features and categorical features. And the dataset should be used to solve the classification problem(should have label, target in this case).\n",
    "  \n",
    "  I think this method can also be applied to other similar datasets. Because for this kind of the non-linear complex model, Logistic Regression with regularization will definitely work well. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental Setup\n",
    "_(Did you use all the data, cross-validation, training / test split, etc? Give enough details on how you setup the experiment so that your colleague can read this section and write their own algorithm to produce the same setup. Provide a link to the cells in the notebooks that contain the experimental setup.)_ 3-4 paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "7124b6b05c26327c785ea4baa791ee03",
     "grade": true,
     "grade_id": "cell-8e284da64965d4fd",
     "locked": false,
     "points": 10,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "  Speaking of the training process, the dataset provided both training set and testing set. \n",
    "  \n",
    "  For the logisitic regression[[3]], I used training/test set split and I set the regularized parameter to 0.1.\n",
    "  \n",
    "  For the training method, I use Stochastic Gradient Descent with mini-batch, I set the batch number to 50 and learning rate to 0.01.\n",
    "  \n",
    "  For the decesion tree, I used K-fold and set K to 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "_(Write a table containing the results of your experiments, which were calculated in the notebooks. Include all algorithms included in questions [3.1](part-3.ipynb#Question-3.1) and above in Part 3 of the notebooks. Provide some interpretation of these results. Do you think you could have done better? If so, why did you not pursue those ideas? Add any pictures you think approprate here.)_ 5-6 paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "47b0ae14537531217347e46db447bbba",
     "grade": true,
     "grade_id": "cell-384dadfd814ac391",
     "locked": false,
     "points": 10,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "  The methods I used and the corresponding results are as follows[[3]]:\n",
    "  \n",
    "| Model               | Features                                                     | Training Method | Normalize or not | Missing Values | Overfitting    | Accuracy |\n",
    "| ------------------- | ------------------------------------------------------------ | --------------- | ---------------- | -------------- | -------------- | -------- |\n",
    "| Linear Regression   | ext_source_1, ext_source_2,ext_source_3                      | mini-batch SGD  | yes              | fill-zero      | Regularization | 0.5437   |\n",
    "| Linear Regression   | 15 features from part 2                                      | mini-batch SGD  | yes              | fill-mean      | Regularization | 0.6582   |\n",
    "| Logistic Regression | 15 features from part 2                                      | mini-batch SGD  | yes              | N/A            | Regularization | 0.6622   |\n",
    "| Logistic Regression | 15 features from part 2                                      | mini-batch SGD  | no               | fill-zero      | Regularization | 0.6703   |\n",
    "| Logistic Regression | ext_source_1, ext_source_2,ext_source_3 and their combinations | mini-batch SGD  | yes              | fill zero      | Regularization | 0.6865   |\n",
    "| Logistic Regression | 15 features from part 2                                      | mini-batch SGD  | yes              | fill mean      | Regularization | 0.7026   |\n",
    "| Desicion Tree       | 15 features from part 2                                      |                 | yes              |                | K-fold         | 0.6735   |\n",
    "\n",
    "\n",
    "  As you can see from the table above, I achieved 0.7026 using all the 15 features from part 2 and regularized logistic regression. What turnes out to be the most important factor is normalization and filling-means. As shown in the table above, if we don't use normalization and filling-means, the accuracy will drop sigificant even if the feature is good enough.\n",
    "  \n",
    "  I saw many of the Kaggler using boosting algorithm, but unfortunately, I don't have time to implement it. Many of the solution with best results are using XGboost or LightBoost algorithm. I think I could have done better using those boosting algorithms.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "_(Summarize your findings. If someone wanted to use your solution, which would you recommend? What could you do if you had more data, etc. What should a company seeking to run this at high scale choose if they were to use your method.)_ 3 paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "bfdbf631dd7ab4a6b983b5f57b05e7d8",
     "grade": true,
     "grade_id": "cell-fac0328f843eb6f5",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "  I recommend Logistic Regression as the model because it's fast and simple and can achieve great outcome as well. However, boosting algorithms will definitely achieve more accuarcy according to the Kaggle ranking. \n",
    "  \n",
    "  The features are very important, in order to find the features that can get higher information gain, we need understand the meanings of each one of the features well.\n",
    "  \n",
    "  The company can get more data run this at a high scale on a distributed system. And I believe it will definitely get better accauracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "_(Add a numbered list of the referenced articles, notebooks, etc. that you cited in the above notebooks. Pay attention that the numbers you used correspond to the list below.)_\n",
    "\n",
    "Consider the MLA or APA style, which should be available in Google Scholar.\n",
    "\n",
    "*Example*\n",
    "\n",
    "[[1]](https://arxiv.org) Bob Smith, John Doe. My amazing method. In _Proceedings of WWW 2018_, Lyon France, 2018.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "97b044cabd4eb3042df6c03c3b625252",
     "grade": true,
     "grade_id": "cell-b608d1f5b8e2570a",
     "locked": false,
     "points": 4,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "[[1]](https://www.kaggle.com/c/home-credit-default-risk/discussion/64821#latest-453304) Bojan Tunguz. 1st Place Solution posted in Home Credit Default Risk, 2018.\n",
    "\n",
    "[[2]](https://www.kaggle.com/jsaguiar/lightgbm-with-simple-features). LightGBM with Simple Features posted in Home Credit Default Risk, 2018.\n",
    "\n",
    "[[3]](https://www.kaggle.com/codename007/home-credit-complete-eda-feature-importance) Shanth. Home Credit : Complete EDA + Feature Importance, 2018.\n",
    "\n",
    "[[4]](https://www.kaggle.com/c/home-credit-default-risk/discussion/57750#latest-374499) Shanth. Feature Engineering with BUREAU DATA - 10 FEATURES posted in Home Credit Default Risk a year ago, 2018.\n",
    "\n",
    "[[5]](http://notebooks.learnml.cool:31757/user/paultzq/notebooks/assignment-5/assignment-5.ipynb) Ziqi Tang. Assignment 5, 2019.\n",
    "\n",
    "[[6]](http://notebooks.learnml.cool:31757/user/paultzq/notebooks/assignment-4/assignment-4.ipynb) Ziqi Tang. Assignment 4, 2019.\n",
    "\n",
    "[[7]] S. Moro, P. Cortez, P. Rita, \"A data-driven approach to predict the success of bank telemarketing\", Decision Support Systems, vol. 62, pp. 22-31, 2014.\n",
    "\n",
    "[[8]] J. Li, H. Wei, W. Hao, \"Weight-selected attribute bagging for credit scoring\", Mathematical Problems in Engineering, vol. 2013, 2013.\n",
    "\n",
    "[[9]]C.-F. Tsai, M.-L. Chen, \"Credit rating by hybrid machine learning techniques\", Applied soft computing, vol. 10, no. 2, pp. 374-380, 2010.\n",
    "\n",
    "[[10]]M. Er, L. Zhai, X. Li, L. San, \"A hybrid online sequential extreme learning machine with simplified hidden network\", IAENG International Journal of Computer Science, vol. 39, no. 1, pp. 1-9, 2012.\n",
    "\n",
    "[[11]]K. Y. Tam, M. Y. Kiang, \"Managerial applications of neural networks: the case of bank failure predictions\", Management science, vol. 38, no. 7, pp. 926-947, 1992."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Appendix\n",
    "\n",
    "Add links to your part 1, part 2, and part 3 notebooks (using absolute links). \n",
    "Add anything else you want to here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "030e49a0552af8832a66591584da9636",
     "grade": true,
     "grade_id": "cell-a941b5ed7957cf69",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "[[Appe.1]](http://notebooks.learnml.cool:31757/user/paultzq/notebooks/final-project-1/part-1.ipynb) Ziqi Tang. Part 1, 2019.\n",
    "\n",
    "[[Appe.2]](http://notebooks.learnml.cool:31757/user/paultzq/notebooks/final-project-2/part-2.ipynb) Ziqi Tang. Part 2, 2019.\n",
    "\n",
    "[[Appe.3]](http://notebooks.learnml.cool:31757/user/paultzq/notebooks/final-project/part-3.ipynb#Final-Project:-Part-3---Classifiers) Ziqi Tang. Part 3, 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ruby 2.5.1",
   "language": "ruby",
   "name": "ruby"
  },
  "language_info": {
   "file_extension": ".rb",
   "mimetype": "application/x-ruby",
   "name": "ruby",
   "version": "2.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
